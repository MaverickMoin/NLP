{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs0kkiefhX6P"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import random\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import transformers\n",
        "import os\n",
        "import sys\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQN0FKXDha2u"
      },
      "outputs": [],
      "source": [
        "classes = 22\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\", num_labels=classes)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGE4nCa9hgpk"
      },
      "outputs": [],
      "source": [
        "# Define unfreeze_layer directly in the notebook\n",
        "unfreeze_layer = 2  # Change this value to unfreeze a different number of layers\n",
        "\n",
        "# Validate the number of layers and unfreeze selected ones\n",
        "layers = len(model.bert.encoder.layer)\n",
        "print(\"Total Layers: \", layers)\n",
        "\n",
        "for layer_no in range(0, layers - unfreeze_layer):\n",
        "    print(f\"Freezing layer : {layer_no}\")\n",
        "    for param in model.bert.encoder.layer[layer_no].parameters():\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyxY7IUihi6k"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "confusion_matrix_mapping  = {\n",
        "    'Assamese' : 0,\n",
        "    'Bangla' : 1,\n",
        "    'Bodo' : 2,\n",
        "    'Konkani' : 3,\n",
        "    'Gujarati' : 4,\n",
        "    'Hindi' : 5,\n",
        "    'Kannada' : 6,\n",
        "    'Kashmiri' : 7,\n",
        "    'Maithili' : 8,\n",
        "    'Malayalam' : 9,\n",
        "    'Manipuri_Mei' : 10,\n",
        "    'Marathi' : 11,\n",
        "    'Nepali' : 12,\n",
        "    'Oriya' : 13,\n",
        "    'Punjabi' : 14,\n",
        "    'Sanskrit' : 15,\n",
        "    'Sindhi' : 16,\n",
        "    'Tamil' : 17,\n",
        "    'Telugu' : 18,\n",
        "    'Urdu' : 19,\n",
        "    'English' : 20,\n",
        "    'Other' : 21\n",
        "}\n",
        "\n",
        "confusion_matrix_reverse_mapping  = {\n",
        "    0 : 'Assamese',\n",
        "    1 : 'Bangla',\n",
        "    2 : 'Bodo',\n",
        "    3 : 'Konkani',\n",
        "    4 : 'Gujarati',\n",
        "    5 : 'Hindi',\n",
        "    6 : 'Kannada',\n",
        "    7 : 'Kashmiri',\n",
        "    8 : 'Maithili',\n",
        "    9 : 'Malayalam',\n",
        "    10 : 'Manipuri_Mei',\n",
        "    11 : 'Marathi',\n",
        "    12 : 'Nepali',\n",
        "    13 : 'Oriya',\n",
        "    14 : 'Punjabi',\n",
        "    15 : 'Sanskrit',\n",
        "    16 : 'Sindhi',\n",
        "    17 : 'Tamil',\n",
        "    18 : 'Telugu',\n",
        "    19 : 'Urdu',\n",
        "    20 : 'English',\n",
        "    21 : 'Other'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbMZ2ikAhpLL"
      },
      "outputs": [],
      "source": [
        "def convert_to_dataframe(file_name):\n",
        "    file_in = open(file_name, 'r')\n",
        "    lines_in = file_in.read().split('\\n')\n",
        "    file_in.close()\n",
        "\n",
        "    print(lines_in[0])\n",
        "\n",
        "    lines_in = [line.split(' ') for line in lines_in if line]\n",
        "    lines_in = [ [ line[0], ' '.join(line[1:]) ] for line in lines_in]\n",
        "\n",
        "    print(lines_in[0])\n",
        "\n",
        "    df = pd.DataFrame(lines_in)\n",
        "\n",
        "    print(df)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptjHicgeuZqZ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ai4bharat/Bhasha-Abhijnaanam\")\n",
        "print(dataset[\"train\"][0])  # Check the structure of a single data point in the training set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku1OZfZqhrgN"
      },
      "outputs": [],
      "source": [
        "df_train = convert_to_dataframe('corpus/train_combine.txt')\n",
        "df_test = convert_to_dataframe('corpus/test_combine.txt')\n",
        "df_valid = convert_to_dataframe('corpus/valid_combine.txt')\n",
        "\n",
        "print(df_train.shape)\n",
        "train_X = df_train.iloc[:, 1]\n",
        "train_y = df_train.iloc[:, 0]\n",
        "\n",
        "test_X = df_test.iloc[:, 1]\n",
        "test_y = df_test.iloc[:, 0]\n",
        "\n",
        "valid_X = df_valid.iloc[:, 1]\n",
        "valid_y = df_valid.iloc[:, 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXwdKvzbhyBr"
      },
      "outputs": [],
      "source": [
        "class DATA(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.size = len(Y)\n",
        "        self.x = X\n",
        "        self.y = Y\n",
        "        # self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        sample = self.x[idx], self.y[idx]\n",
        "\n",
        "        text, label = sample[0], sample[1]\n",
        "\n",
        "        # if self.transform:\n",
        "        #     sample = self.transform(sample)\n",
        "        target = confusion_matrix_mapping[ label[9:] ]\n",
        "\n",
        "        return tuple([text, target])\n",
        "\n",
        "\n",
        "def get_dataloaders(X_train, X_test, X_valid, y_train, y_test, y_valid, batch_size):\n",
        "\n",
        "    train = DATA(X_train, y_train)\n",
        "    test = DATA(X_test, y_test)\n",
        "    valid = DATA(X_valid, y_valid)\n",
        "\n",
        "\n",
        "    train_dl = torch.utils.data.DataLoader(train,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=True\n",
        "                                            )\n",
        "\n",
        "    test_dl = torch.utils.data.DataLoader(test,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=False\n",
        "                                            )\n",
        "\n",
        "    valid_dl = torch.utils.data.DataLoader(valid,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=False\n",
        "                                            )\n",
        "\n",
        "    return train_dl, test_dl, valid_dl\n",
        "\n",
        "train_dataloader, test_dataloader, valid_dataloader = get_dataloaders(train_X, test_X, valid_X, train_y, test_y, valid_y, batch_size = 64)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir result_unfreeze"
      ],
      "metadata": {
        "id": "eUe0B8IPFLJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFnGHGpchOxJ"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00003, weight_decay=1e-5)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=400, gamma=0.1)\n",
        "scheduler = transformers.get_linear_schedule_with_warmup( optimizer, num_warmup_steps=1500, num_training_steps=60000 )\n",
        "\n",
        "\n",
        "print('start training')\n",
        "train_acc, valid_acc = [], []\n",
        "\n",
        "i=0\n",
        "epochs = 1\n",
        "max_val_acc = 0.0\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    running_loss_train = 0.0\n",
        "    running_loss_valid = 0.0\n",
        "    print(epoch)\n",
        "    for data_train in train_dataloader:\n",
        "        i+=1\n",
        "\n",
        "        inputs_train, labels_train = data_train[0], data_train[1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        word_embeddings_train = tokenizer(inputs_train, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        # print(word_embeddings_train.keys())\n",
        "\n",
        "        labels_train = labels_train.to(device)\n",
        "        word_embeddings_train = word_embeddings_train.to(device)\n",
        "\n",
        "        outputs_train = model(word_embeddings_train['input_ids'],\n",
        "                             token_type_ids=word_embeddings_train['token_type_ids'],\n",
        "                             attention_mask=word_embeddings_train['attention_mask'],\n",
        "                             labels=labels_train)\n",
        "\n",
        "        loss_train = outputs_train.loss\n",
        "\n",
        "        loss_train.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss_train += loss_train.item()\n",
        "\n",
        "        if i % 100 == 0:    # print every 2000 mini-batches\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for data_valid in valid_dataloader:\n",
        "                    inputs_valid, labels_valid = data_valid[0], data_valid[1]\n",
        "\n",
        "                    word_embeddings_valid = tokenizer(inputs_valid, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "                    word_embeddings_valid = word_embeddings_valid.to(device)\n",
        "                    labels_valid = labels_valid.to(device)\n",
        "\n",
        "                    outputs_valid = model(word_embeddings_valid['input_ids'],\n",
        "                             token_type_ids=word_embeddings_valid['token_type_ids'],\n",
        "                             attention_mask=word_embeddings_valid['attention_mask'],\n",
        "                             labels=labels_valid)\n",
        "\n",
        "                    loss_valid = outputs_valid.loss\n",
        "                    # loss_valid =  criterion(outputs_valid['logits'], labels_valid)\n",
        "\n",
        "                    running_loss_valid += loss_valid.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs_valid.logits, 1)\n",
        "\n",
        "                    total += labels_valid.size(0)\n",
        "                    correct += (predicted == labels_valid).sum().item()\n",
        "\n",
        "                curr_val_acc = ((100 * correct) / total)\n",
        "                if curr_val_acc > max_val_acc:\n",
        "                    if max_val_acc:\n",
        "                        os.remove('result_unfreeze/basline_nn_simple.pt')\n",
        "                    torch.save(model, 'result_unfreeze/basline_nn_simple.pt')\n",
        "                    max_val_acc = curr_val_acc\n",
        "\n",
        "            print(f'Accuracy of the network on the {total} valid inputs: {100 * correct / total} ')\n",
        "            file_log = open('result_unfreeze/acc_log.txt', 'a')\n",
        "            file_log.write(f'Accuracy of the network on the {total} valid inputs: {100 * correct / total} '+'\\n')\n",
        "\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] Training loss: {running_loss_train / 100:.3f}')\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] Valid loss: {running_loss_valid}')\n",
        "            file_log.write(f'[{epoch + 1}, {i + 1:5d}] Training loss: {running_loss_train / 100:.3f}'+'\\n')\n",
        "            file_log.write(f'[{epoch + 1}, {i + 1:5d}] Valid loss: {running_loss_valid}'+'\\n')\n",
        "            file_log.close()\n",
        "\n",
        "            running_loss_train = 0.0\n",
        "            running_loss_valid = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "print(max(train_acc), max(test_acc))\n",
        "torch.save(net, 'result_unfreeze/basline_nn_simple.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}