{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUs63KOEBSiz"
      },
      "outputs": [],
      "source": [
        "!pip3 install fasttext\n",
        "!pip3 install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gDhqkoTnUHbh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!git clone https://github.com/AI4Bharat/IndicLID.git\n",
        "%cd \"/content/IndicLID/Inference\"\n",
        "%mkdir models\n",
        "%cd \"/content/IndicLID/Inference/models\"\n",
        "\n",
        "\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-bert.zip\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-ftn.zip\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-ftr.zip\n",
        "!unzip indiclid-bert.zip\n",
        "!unzip indiclid-ftn.zip\n",
        "!unzip indiclid-ftr.zip\n",
        "\n",
        "\n",
        "# %cd \"/content/IndicLID/\"\n",
        "%cd \"/content/IndicLID/\"\n",
        "%mkdir train_data\n",
        "%cd \"/content/IndicLID/train_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaaE5Dv7chkF"
      },
      "outputs": [],
      "source": [
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/as.txt | head -n 10000 > as.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/bd.txt | head -n 10000 > bd.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/bn.txt | head -n 10000 > bn.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/en.txt | head -n 10000 > en.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/gom.txt | head -n 10000 > gom.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/gu.txt | head -n 10000 > gu.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/hi.txt | head -n 10000 > hi.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/kn.txt | head -n 10000 > kn.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ks.txt | head -n 10000 > ks.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mai.txt | head -n 10000 > mai.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ml.txt | head -n 10000 > ml.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mni.txt | head -n 10000 > mni.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mr.txt | head -n 10000 > mr.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ne.txt | head -n 10000 > ne.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/or.txt | head -n 10000 > or.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/pa.txt | head -n 10000 > pa.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/sa.txt | head -n 10000 > sa.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/sd.txt | head -n 10000 > sd.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ta.txt | head -n 10000 > ta.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/te.txt | head -n 10000 > te.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ur.txt | head -n 10000 > ur.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NATIVE DATA"
      ],
      "metadata": {
        "id": "-1MV0ax8YNzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/IndicLID/train_data\n"
      ],
      "metadata": {
        "id": "w383dJCGaa7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_code_dict_ntv = {\n",
        "    'as' : 'asm_Beng',\n",
        "    'bn' : 'ben_Beng',\n",
        "    'bd' : 'brx_Deva',\n",
        "    'gom' : 'kok_Deva',\n",
        "    'gu' : 'guj_Gujr',\n",
        "    'hi' : 'hin_Deva',\n",
        "    'kn' : 'kan_Knda',\n",
        "\t'ks' : 'kas_Arab',\n",
        "    'mai': 'mai_Deva',\n",
        "    'ml' : 'mal_Mlym',\n",
        "\t'mni' : 'mni_Meti',\n",
        "    'mr' : 'mar_Deva',\n",
        "    'ne' : 'nep_Deva',\n",
        "    'or' : 'ori_Orya',\n",
        "    'pa' : 'pan_Guru',\n",
        "    'sa' : 'san_Deva',\n",
        "    'sd' : 'snd_Arab',\n",
        "    'ta' : 'tam_Tamil',\n",
        "    'te' : 'tel_Telu',\n",
        "    'ur' : 'urd_Arab',\n",
        "    'en' : 'English',\n",
        "}\n",
        "\n",
        "for code, lang in lang_code_dict_ntv.items():\n",
        "\n",
        "\t# Traverse the file line by line and transliterate\n",
        "\tfile_path = f\"/content/IndicLID/train_data/{code}.txt\"\n",
        "\toutput_file_path = f\"/content/IndicLID/train_data/{code}ntv_train.txt\"\n",
        "\toutput_file = open(output_file_path, 'w', encoding='utf-8')\n",
        "\ti=0\n",
        "\t# Read the file and transliterate each line\n",
        "\twith open(file_path, 'r', encoding='utf-8') as file:\n",
        "\t\tfor line in file:\n",
        "\t\t\tif(i<20000):\n",
        "\n",
        "\t\t\t\t# Strip any whitespace or newline characters from the line\n",
        "\t\t\t\tline = line.strip()\n",
        "\t\t\t\ti += 1\n",
        "\n",
        "\t\t\t\t# Transliterate the line using the engine\n",
        "\t\t\t\tif line:  # Ensure it's not an empty line\n",
        "\t\t\t\t\toutput_file.write(f\"__label__{lang} {line}\\n\")\n",
        "\t\t\t\t\t#print(f\"Input: {line}\")\n",
        "\t\t\t\t\t#print(f\"__label__{lang} {output}\")\n",
        "\n",
        "\toutput_file.close()  # Manually closing the file after the loop\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xTeR1LmeYPl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/IndicLID\n",
        "%mkdir train\n",
        "%mkdir valid\n"
      ],
      "metadata": {
        "id": "dJ8dt19Cb-Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/IndicLID/train_data\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# List all files in the current directory that end with \"ntv_train.txt\"\n",
        "all_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith(\"ntv_train.txt\")]\n",
        "\n",
        "# Traverse through each file\n",
        "for file_name in all_files:\n",
        "    with open(file_name, 'r') as file:\n",
        "        data = file.read()\n",
        "        lines = data.splitlines()\n",
        "\n",
        "        # Only proceed if lines are not empty\n",
        "        if lines:\n",
        "            train_lines, valid_lines = train_test_split(lines, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Write the training data to a new file\n",
        "            train_file_name = f\"../train/{file_name}\"\n",
        "            with open(train_file_name, 'w') as train_file:\n",
        "                train_file.write(\"\\n\".join(train_lines))\n",
        "\n",
        "            # Write the validation data to a new file\n",
        "            valid_file_name = f\"../valid/{file_name}\"\n",
        "            with open(valid_file_name, 'w') as valid_file:\n",
        "                valid_file.write(\"\\n\".join(valid_lines))\n",
        "        else:\n",
        "            print(f\"Skipping empty file: {file_name}\")\n"
      ],
      "metadata": {
        "id": "Sue9ZjpybVIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "model = fasttext.load_model('/content/IndicLID/Inference/models/indiclid-ftn/model_baseline_roman.bin')"
      ],
      "metadata": {
        "id": "zubIOTg_bLI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/IndicLID/train\n",
        "!cat *train.txt > train_combine.txt\n",
        "\n",
        "%cd /content/IndicLID/valid\n",
        "!cat *train.txt > test_combine.txt"
      ],
      "metadata": {
        "id": "343r1K3idxxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data= f\"/content/IndicLID/train/train_combine.txt\"\n",
        "print(train_data)\n",
        "model = fasttext.train_supervised(\n",
        "    input = train_data,\n",
        "    lr = 0.3,\n",
        "    dim = 512,\n",
        "    ws = 5,\n",
        "    epoch = 3,\n",
        "    minCount = 1,\n",
        "    minCountLabel = 0,\n",
        "    minn = 3,\n",
        "    maxn = 6,\n",
        "    neg = 5,\n",
        "    wordNgrams = 2,\n",
        "    loss = 'hs',\n",
        "    lrUpdateRate = 100,\n",
        "    t = 0.0001,\n",
        "    verbose = 1\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "EduUc6FYdwoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/IndicLID/\"\n",
        "%mkdir finetuned\n",
        "model.save_model(\"/content/IndicLID/finetuned/model_baseline_native.bin\")"
      ],
      "metadata": {
        "id": "c0UXTAXNeLiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROMAN DATA"
      ],
      "metadata": {
        "id": "WXg8CMg1YME2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw_vpyXMF95r"
      },
      "outputs": [],
      "source": [
        "%cd /content/IndicLID\n",
        "%mkdir results\n",
        "%mkdir results_ntv\n",
        "!cp /content/IndicLID/valid/test_combine.txt /content/test_combine.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import sys\n",
        "import fasttext\n",
        "\n",
        "IndicPath='/content/IndicLID/Inference/models/indiclid-ftn/model_baseline_roman.bin'\n",
        "FinetunePathntv='/content/IndicLID/finetuned/model_baseline_native.bin'\n",
        "model = fasttext.load_model(FinetunePathntv)\n",
        "\n",
        "file = open('/content/IndicLID/results_ntv/result.txt', 'w')\n",
        "\n",
        "lines_config = []\n",
        "\n",
        "lines_config.append('lr : ' + str ( model.f.getArgs().lr ) )\n",
        "lines_config.append('dim : ' + str ( model.f.getArgs().dim ) )\n",
        "lines_config.append('ws : ' + str ( model.f.getArgs().ws ) )\n",
        "lines_config.append('epoch : ' + str ( model.f.getArgs().epoch ) )\n",
        "lines_config.append('minCount : ' + str ( model.f.getArgs().minCount ) )\n",
        "lines_config.append('minCountLabel : ' + str ( model.f.getArgs().minCountLabel ) )\n",
        "lines_config.append('minn : ' + str ( model.f.getArgs().minn ) )\n",
        "lines_config.append('maxn : ' + str ( model.f.getArgs().maxn ) )\n",
        "lines_config.append('neg : ' + str ( model.f.getArgs().neg ) )\n",
        "lines_config.append('wordNgrams : ' + str ( model.f.getArgs().wordNgrams ) )\n",
        "lines_config.append('loss : ' + str ( model.f.getArgs().loss ) )\n",
        "lines_config.append('bucket : ' + str ( model.f.getArgs().bucket ) )\n",
        "lines_config.append('thread : ' + str ( model.f.getArgs().thread ) )\n",
        "lines_config.append('lrUpdateRate : ' + str ( model.f.getArgs().lrUpdateRate ) )\n",
        "lines_config.append('t : ' + str ( model.f.getArgs().t ) )\n",
        "lines_config.append('label : ' + str ( model.f.getArgs().label ) )\n",
        "lines_config.append('verbose : ' + str ( model.f.getArgs().verbose ) )\n",
        "lines_config.append('pretrainedVectors : ' + str ( model.f.getArgs().pretrainedVectors ) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "file.write('Hyperparameters (Tuned)')\n",
        "file.write('\\n')\n",
        "file.write('\\n'.join(lines_config))\n",
        "file.write('\\n\\n\\n\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "result_train = model.test(\"/content/test_combine.txt\")\n",
        "\n",
        "file.write('Evaluation Results Train Set')\n",
        "file.write('\\n')\n",
        "file.write('train Set')\n",
        "file.write('\\n')\n",
        "file.write('Samples : ' + str(result_train[0]) )\n",
        "file.write('\\n')\n",
        "file.write('precision : ' + str(result_train[1]) )\n",
        "file.write('\\n')\n",
        "file.write('recall : ' + str(result_train[2]) )\n",
        "file.write('\\n')\n",
        "file.write('F1-Score : ' + str( (2 * result_train[1] * result_train[2]) / (result_train[1] + result_train[2]) ) )\n",
        "file.write('\\n\\n\\n')\n",
        "\n",
        "\n",
        "confusion_matrix_reverse_mapping  = {\n",
        "    0 : 'Assamese',\n",
        "    1 : 'Bangla',\n",
        "    2 : 'Bodo',\n",
        "    3 : 'Konkani',\n",
        "    4 : 'Gujarati',\n",
        "    5 : 'Hindi',\n",
        "    6 : 'Kannada',\n",
        "    7 : 'Kashmiri',\n",
        "    22 : 'Kashmiri_Deva',\n",
        "    8 : 'Maithili',\n",
        "    9 : 'Malayalam',\n",
        "    23 : 'Manipuri_Mei',\n",
        "    10 : 'Manipuri',\n",
        "    11 : 'Marathi',\n",
        "    12 : 'Nepali',\n",
        "    13 : 'Oriya',\n",
        "    14 : 'Punjabi',\n",
        "    15 : 'Sanskrit',\n",
        "    16 : 'Sindhi',\n",
        "    17 : 'Tamil',\n",
        "    18 : 'Telugu',\n",
        "    19 : 'Urdu',\n",
        "    20 : 'English',\n",
        "    21 : 'Other'\n",
        "}\n",
        "\n",
        "confusion_matrix_mapping  = {\n",
        "    'Assamese' : 0,\n",
        "    'Bangla' : 1,\n",
        "    'Bodo' : 2,\n",
        "    'Konkani' : 3,\n",
        "    'Gujarati' : 4,\n",
        "    'Hindi' : 5,\n",
        "    'Kannada' : 6,\n",
        "    'Kashmiri' : 7,\n",
        "    'Kashmiri_Deva' : 22,\n",
        "    'Maithili' : 8,\n",
        "    'Malayalam' : 9,\n",
        "    'Manipuri' : 10,\n",
        "    'Manipuri_Mei' : 23,\n",
        "    'Marathi' : 11,\n",
        "    'Nepali' : 12,\n",
        "    'Oriya' : 13,\n",
        "    'Punjabi' : 14,\n",
        "    'Sanskrit' : 15,\n",
        "    'Sindhi' : 16,\n",
        "    'Tamil' : 17,\n",
        "    'Telugu' : 18,\n",
        "    'Urdu' : 19,\n",
        "    'English' : 20,\n",
        "    'Other' : 21\n",
        "}\n",
        "\n",
        "IndicLID_lang_code_dict = {\n",
        "    'asm_Beng' : 0,\n",
        "    'ben_Beng' : 1,\n",
        "    'brx_Deva' : 2,\n",
        "    'kok_Deva' : 3,\n",
        "    'guj_Gujr' : 4,\n",
        "    'hin_Deva' : 5,\n",
        "    'kan_Knda' : 6,\n",
        "    'kas_Arab' : 7,\n",
        "    'kas_Deva' : 22,\n",
        "    'mai_Deva' : 8,\n",
        "    'mal_Mlym' : 9,\n",
        "    'mni_Meti' : 10,\n",
        "    'mni_Beng' : 23,\n",
        "    'mar_Deva' : 11,\n",
        "    'nep_Deva' : 12,\n",
        "    'ori_Orya' : 13,\n",
        "    'pan_Guru' : 14,\n",
        "    'san_Deva' : 15,\n",
        "    'snd_Arab' : 16,\n",
        "    'tam_Tamil': 17,\n",
        "    'tel_Telu' : 18,\n",
        "    'urd_Arab' : 19,\n",
        "    'eng_Latn' : 20,\n",
        "    'other'    : 21,\n",
        "    'doi_Deva' : 21,\n",
        "    'sat_Olch' : 21,\n",
        "    'English' : 20,\n",
        "    'Other' : 21\n",
        "}\n",
        "\n",
        "IndicLID_lang_code_dict_reverse = {\n",
        "     0  : 'asm_Beng' ,\n",
        "     1  : 'ben_Beng' ,\n",
        "     2  : 'brx_Deva' ,\n",
        "     3  : 'kok_Deva' ,\n",
        "     4  : 'guj_Gujr' ,\n",
        "     5  : 'hin_Deva' ,\n",
        "     6  : 'kan_Knda' ,\n",
        "     7  : 'kas_Arab' ,\n",
        "     22  : 'kas_Deva' ,\n",
        "     8  : 'mai_Deva' ,\n",
        "     9  : 'mal_Mlym' ,\n",
        "     10 : 'mni_Meti' ,\n",
        "     23 : 'mni_Beng' ,\n",
        "     11 : 'mar_Deva' ,\n",
        "     12 : 'nep_Deva' ,\n",
        "     13 : 'ori_Orya' ,\n",
        "     14 : 'pan_Guru' ,\n",
        "     15 : 'san_Deva' ,\n",
        "     16 : 'snd_Arab' ,\n",
        "     17 : 'tam_Tamil',\n",
        "     18 : 'tel_Telu' ,\n",
        "     19 : 'urd_Arab' ,\n",
        "     20 : 'eng_Latn' ,\n",
        "     21 : 'other'    ,\n",
        "\t }\n",
        "\n",
        "\n",
        "def evaluate(test_file_name):\n",
        "\n",
        "    classes = 23\n",
        "\n",
        "    # inference for Dakshina, test_combine_dakshina\n",
        "    file_test = open('/content/'+test_file_name+'.txt', 'r')\n",
        "    lines_test = file_test.read().split('\\n')\n",
        "    file_test.close()\n",
        "\n",
        "\n",
        "    # save predictions\n",
        "    file_predictions = open('/content/IndicLID/results_ntv/predictions_'+test_file_name+'.csv', 'w')\n",
        "    csv_writer_predictions = csv.writer(file_predictions)\n",
        "    csv_writer_predictions.writerow( [ 'Sentence', 'Ground truth', 'Prediction', 'Score' ] )\n",
        "\n",
        "    file_predictions_right = open('/content/IndicLID/results_ntv/right_predictions_'+test_file_name+'.csv', 'w')\n",
        "    csv_writer_predictions_right = csv.writer(file_predictions_right)\n",
        "    csv_writer_predictions_right.writerow( [ 'Sentence', 'Ground truth', 'Prediction', 'Score' ] )\n",
        "\n",
        "\n",
        "    file_predictions_wrong = open('/content/IndicLID/results_ntv/wrong_predictions_'+test_file_name+'.csv', 'w')\n",
        "    csv_writer_predictions_wrong = csv.writer(file_predictions_wrong)\n",
        "    csv_writer_predictions_wrong.writerow( [ 'Sentence', 'Ground truth', 'Prediction', 'Score' ] )\n",
        "\n",
        "\n",
        "\n",
        "    # Computing confusion matrix\n",
        "    confusion_matrix = []\n",
        "    for i in range(classes):\n",
        "        confusion_matrix.append( [0]*classes )\n",
        "\n",
        "    # to calculate accuracy and save right and wrong prediction\n",
        "    count = 0\n",
        "    n = 0\n",
        "    for line in lines_test:\n",
        "        label = line.split(' ')[0]\n",
        "        sen = ' '.join(line.split(' ')[1:])\n",
        "        pred_label = model.predict(sen)[0][0]\n",
        "        pred_score = model.predict(sen)[1][0]\n",
        "\n",
        "        if pred_label == label:\n",
        "            count+=1\n",
        "            csv_writer_predictions_right.writerow( [ sen, label, pred_label, pred_score ] )\n",
        "        else:\n",
        "            csv_writer_predictions_wrong.writerow( [ sen, label, pred_label, pred_score ] )\n",
        "        n+=1\n",
        "        #print(\"This is label : \"+label)\n",
        "        #print(\"This is label : \"+pred_label)\n",
        "\n",
        "\n",
        "        if len(label[9:])>0:\n",
        "          print(\"This file label : \"+label[9:])\n",
        "          print(IndicLID_lang_code_dict[label[9:]] )\n",
        "          print(\"This model label : \"+pred_label[9:])\n",
        "          print(IndicLID_lang_code_dict[pred_label[9:]] )\n",
        "          confusion_matrix[ IndicLID_lang_code_dict[label[9:]] ][ IndicLID_lang_code_dict[pred_label[9:]] ] += 1\n",
        "          csv_writer_predictions.writerow( [ sen, label, pred_label, pred_score ] )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Computing precision, recall and f1\n",
        "    precsison_recall_f1 = []\n",
        "    for i in range(classes):\n",
        "        precsison_recall_f1.append([0] * 3)\n",
        "\n",
        "    precision_denominator = 0\n",
        "    recall_denominator = 0\n",
        "    f1_denominator = 0\n",
        "\n",
        "    for i in range(classes):\n",
        "        no_of_correctly_predicted = confusion_matrix[i][i]\n",
        "        total_predictions_as_i = 0\n",
        "\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1_value = 0\n",
        "\n",
        "        # true predicted i values out of all predicted i values\n",
        "        for j in range(classes):\n",
        "            total_predictions_as_i += confusion_matrix[j][i]\n",
        "        if (total_predictions_as_i != 0):\n",
        "            precision = no_of_correctly_predicted/total_predictions_as_i\n",
        "            precision_denominator += 1\n",
        "\n",
        "        # true predicted i values out of all actual i values\n",
        "        total_actual_values_of_i = sum(confusion_matrix[i])\n",
        "        if (total_actual_values_of_i != 0):\n",
        "            recall = no_of_correctly_predicted/total_actual_values_of_i\n",
        "            recall_denominator += 1\n",
        "\n",
        "        # f1 score\n",
        "        if (precision + recall != 0):\n",
        "            f1_value = (2 * precision * recall) / (precision + recall)\n",
        "            f1_denominator += 1\n",
        "\n",
        "        precsison_recall_f1[i][0] = precision\n",
        "        precsison_recall_f1[i][1] = recall\n",
        "        precsison_recall_f1[i][2] = f1_value\n",
        "\n",
        "    avg_precision = sum([precsison_recall_f1[i][0] for i in range(classes)]) / precision_denominator\n",
        "    avg_recall = sum([precsison_recall_f1[i][1] for i in range(classes)]) / recall_denominator\n",
        "    avg_f1_score = sum([precsison_recall_f1[i][2] for i in range(classes)]) / f1_denominator\n",
        "\n",
        "\n",
        "\n",
        "    # to save confusion matrix and precision recall matrix\n",
        "\n",
        "    for i in range(classes):\n",
        "        precsison_recall_f1[i].insert(0, confusion_matrix_reverse_mapping[i])\n",
        "\n",
        "    precsison_recall_f1.insert( 0, ['', 'precision', 'recall', 'f1'])\n",
        "    precsison_recall_f1.append( ['Avg', avg_precision, avg_recall, avg_f1_score] )\n",
        "\n",
        "\n",
        "    file_precision_recall_f1 = open('/content/IndicLID/results_ntv/precision_recall_f1_'+test_file_name+'.csv', 'w')\n",
        "    precision_recall_f1_csv_writer = csv.writer(file_precision_recall_f1)\n",
        "\n",
        "    for i in range(classes+2):\n",
        "        precision_recall_f1_csv_writer.writerow(precsison_recall_f1[i])\n",
        "    file_precision_recall_f1.close()\n",
        "\n",
        "\n",
        "\n",
        "    # save confusion matrix\n",
        "    for i in range(classes):\n",
        "        confusion_matrix[i].insert(0, confusion_matrix_reverse_mapping[i] )\n",
        "\n",
        "    confusion_matrix.insert( 0, [''] + [confusion_matrix_reverse_mapping[i] for i in range(classes)] )\n",
        "\n",
        "\n",
        "    file_confusion_matrix = open('/content/IndicLID/results_ntv/confusion_matrix_'+test_file_name+'.csv', 'w')\n",
        "    confusion_matrix_csv_writer = csv.writer(file_confusion_matrix)\n",
        "\n",
        "    for i in range(classes+1):\n",
        "        confusion_matrix_csv_writer.writerow(confusion_matrix[i])\n",
        "\n",
        "\n",
        "    file_confusion_matrix.close()\n",
        "    file_predictions.close()\n",
        "    file_predictions_right.close()\n",
        "    file_predictions_wrong.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # fasttext evaluation scores\n",
        "    test_acc = count/n\n",
        "    result_test = model.test('/content/test_combine.txt')\n",
        "    file.write('fasttext evaluation scores - ' + test_file_name + '\\n')\n",
        "    file.write('Samples : ' + str(result_test[0]) + '\\n')\n",
        "    file.write('precision : ' + str(result_test[1]) + '\\n')\n",
        "    file.write('recall : ' + str(result_test[2]) + '\\n')\n",
        "    file.write('F1-Score : ' + str( (2 * result_test[1] * result_test[2]) / (result_test[1] + result_test[2]) ) + '\\n')\n",
        "    file.write('Accuracy : ' + str(test_acc) + '\\n')\n",
        "    file.write('\\n\\n\\n')\n",
        "\n",
        "evaluate('test_combine')\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "id": "ZI2FKq1ZfIzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Row normalization\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the confusion matrix from a CSV file\n",
        "file_path = '/content/IndicLID/results_ntv/confusion_matrix_test_combine.csv'  # Update with your file path\n",
        "confusion_matrix_df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the dataframe to set the first column as an index (assuming it contains row labels)\n",
        "confusion_matrix = confusion_matrix_df.set_index(confusion_matrix_df.columns[0])\n",
        "\n",
        "confusion_matrix_row_norm = confusion_matrix.div(confusion_matrix.sum(axis=1), axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(confusion_matrix_row_norm, annot=False, cmap=\"Blues\", cbar=True)\n",
        "plt.title(\"Normalized Confusion Matrix Heatmap\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mpD5TaobVqmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuTovpKbVVqp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the confusion matrix from a CSV file\n",
        "file_path = '/content/IndicLID/results_ntv/confusion_matrix_test_combine.csv'  # Update with your file path\n",
        "confusion_matrix_df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the dataframe to set the first column as an index (assuming it contains row labels)\n",
        "confusion_matrix = confusion_matrix_df.set_index(confusion_matrix_df.columns[0])\n",
        "\n",
        "# Plotting the heatmap using Seaborn\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(confusion_matrix, annot=False, cmap=\"Blues\", cbar=True)\n",
        "plt.title(\"Confusion Matrix Heatmap (Shades)\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-Vt633lWT2H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "file_path = '/content/IndicLID/results_ntv/precision_recall_f1_test_combine.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)  # Load data into DataFrame\n",
        "data.columns = [column.upper() for column in data.columns]\n",
        "\n",
        "# Display the data as a styled table with solid borders\n",
        "styled_table = data.style.set_table_styles(\n",
        "    [{'selector': 'table', 'props': [('border-collapse', 'collapse')]},\n",
        "     {'selector': 'th', 'props': [('border', '1px solid black'), ('padding', '8px')]},\n",
        "     {'selector': 'td', 'props': [('border', '1px solid black'), ('padding', '8px')]}]\n",
        ").set_properties(**{'text-align': 'center'})\n",
        "\n",
        "# Render the styled table\n",
        "styled_table\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}