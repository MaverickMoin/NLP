{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpE-KhbjCXeG"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmYy3nqlCJRu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBvfkzbkFop-"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AI4Bharat/IndicLID.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH4sd4fqGOor"
      },
      "source": [
        "IMPORTING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBDR5wkNF_tU"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/IndicLID/Inference\"\n",
        "%mkdir models\n",
        "%cd \"/content/IndicLID/Inference/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vJbuFlBFvts"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-bert.zip\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-ftn.zip\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-ftr.zip\n",
        "!unzip indiclid-bert.zip\n",
        "!unzip indiclid-ftn.zip\n",
        "!unzip indiclid-ftr.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io4kf0jMF1Az"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/IndicLID/Inference\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "\n",
        "import fasttext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import transformers\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "\n",
        "class IndicBERT_Data(Dataset):\n",
        "    def __init__(self, indices, X):\n",
        "        self.size = len(X)\n",
        "        self.x = X\n",
        "        self.i = indices\n",
        "\n",
        "        # self.y = Y\n",
        "        # self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(self.x)\n",
        "\n",
        "        text = self.x[idx]\n",
        "        # text = sample[0]\n",
        "\n",
        "        index = self.i[idx]\n",
        "\n",
        "\n",
        "\n",
        "        # if self.transform:\n",
        "        #     sample = self.transform(sample)\n",
        "        # target = self.IndicLID_lang_code_dict[ label[9:] ]\n",
        "\n",
        "        return tuple([index, text])\n",
        "\n",
        "\n",
        "class IndicLID():\n",
        "\n",
        "    def __init__(self, input_threshold = 0.5, roman_lid_threshold = 0.6):\n",
        "        # define dictionary for roman and native languages to langauge code\n",
        "        # define input_threhsold percentage for native and roman script input diversion\n",
        "        # define model_threhsold for roman script model\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.IndicLID_FTN_path = '/content/IndicLID/finetuned/model_baseline_native.bin'\n",
        "        self.IndicLID_FTR_path = '/content/IndicLID/finetuned/model_baseline_roman.bin'\n",
        "        self.IndicLID_BERT_path = 'models/indiclid-bert/basline_nn_simple.pt'\n",
        "\n",
        "        self.IndicLID_FTN = fasttext.load_model(self.IndicLID_FTN_path)\n",
        "        self.IndicLID_FTR = fasttext.load_model(self.IndicLID_FTR_path)\n",
        "        self.IndicLID_BERT = torch.load(self.IndicLID_BERT_path, map_location = self.device)\n",
        "        self.IndicLID_BERT.eval()\n",
        "        self.IndicLID_BERT_tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\")\n",
        "\n",
        "        self.input_threshold = input_threshold\n",
        "        self.model_threshold = roman_lid_threshold\n",
        "        self.classes = 47\n",
        "\n",
        "        self.IndicLID_lang_code_dict = {\n",
        "            'asm_Latn' : 0,\n",
        "            'ben_Latn' : 1,\n",
        "            'brx_Latn' : 2,\n",
        "            'guj_Latn' : 3,\n",
        "            'hin_Latn' : 4,\n",
        "            'kan_Latn' : 5,\n",
        "            'kas_Latn' : 6,\n",
        "            'kok_Latn' : 7,\n",
        "            'mai_Latn' : 8,\n",
        "            'mal_Latn' : 9,\n",
        "            'mni_Latn' : 10,\n",
        "            'mar_Latn' : 11,\n",
        "            'nep_Latn' : 12,\n",
        "            'ori_Latn' : 13,\n",
        "            'pan_Latn' : 14,\n",
        "            'san_Latn' : 15,\n",
        "            'snd_Latn' : 16,\n",
        "            'tam_Latn' : 17,\n",
        "            'tel_Latn' : 18,\n",
        "            'urd_Latn' : 19,\n",
        "            'eng_Latn' : 20,\n",
        "            'other' : 21,\n",
        "            'asm_Beng' : 22,\n",
        "            'ben_Beng' : 23,\n",
        "            'brx_Deva' : 24,\n",
        "            'doi_Deva' : 25,\n",
        "            'guj_Gujr' : 26,\n",
        "            'hin_Deva' : 27,\n",
        "            'kan_Knda' : 28,\n",
        "            'kas_Arab' : 29,\n",
        "            'kas_Deva' : 30,\n",
        "            'kok_Deva' : 31,\n",
        "            'mai_Deva' : 32,\n",
        "            'mal_Mlym' : 33,\n",
        "            'mni_Beng' : 34,\n",
        "            'mni_Meti' : 35,\n",
        "            'mar_Deva' : 36,\n",
        "            'nep_Deva' : 37,\n",
        "            'ori_Orya' : 38,\n",
        "            'pan_Guru' : 39,\n",
        "            'san_Deva' : 40,\n",
        "            'sat_Olch' : 41,\n",
        "            'snd_Arab' : 42,\n",
        "            'tam_Tamil' : 43,\n",
        "            'tel_Telu' : 44,\n",
        "            'urd_Arab' : 45\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "        self.IndicLID_lang_code_dict_reverse = {\n",
        "            0 : 'asm_Latn',\n",
        "            1 : 'ben_Latn',\n",
        "            2 : 'brx_Latn',\n",
        "            3 : 'guj_Latn',\n",
        "            4 : 'hin_Latn',\n",
        "            5 : 'kan_Latn',\n",
        "            6 : 'kas_Latn',\n",
        "            7 : 'kok_Latn',\n",
        "            8 : 'mai_Latn',\n",
        "            9 : 'mal_Latn',\n",
        "            10 : 'mni_Latn',\n",
        "            11 : 'mar_Latn',\n",
        "            12 : 'nep_Latn',\n",
        "            13 : 'ori_Latn',\n",
        "            14 : 'pan_Latn',\n",
        "            15 : 'san_Latn',\n",
        "            16 : 'snd_Latn',\n",
        "            17 : 'tam_Latn',\n",
        "            18 : 'tel_Latn',\n",
        "            19 : 'urd_Latn',\n",
        "            20 : 'eng_Latn',\n",
        "            21 : 'other',\n",
        "            22 : 'asm_Beng',\n",
        "            23 : 'ben_Beng',\n",
        "            24 : 'brx_Deva',\n",
        "            25 : 'doi_Deva',\n",
        "            26 : 'guj_Gujr',\n",
        "            27 : 'hin_Deva',\n",
        "            28 : 'kan_Knda',\n",
        "            29 : 'kas_Arab',\n",
        "            30 : 'kas_Deva',\n",
        "            31 : 'kok_Deva',\n",
        "            32 : 'mai_Deva',\n",
        "            33 : 'mal_Mlym',\n",
        "            34 : 'mni_Beng',\n",
        "            35 : 'mni_Meti',\n",
        "            36 : 'mar_Deva',\n",
        "            37 : 'nep_Deva',\n",
        "            38 : 'ori_Orya',\n",
        "            39 : 'pan_Guru',\n",
        "            40 : 'san_Deva',\n",
        "            41 : 'sat_Olch',\n",
        "            42 : 'snd_Arab',\n",
        "            43 : 'tam_Tamil',\n",
        "            44 : 'tel_Telu',\n",
        "            45 : 'urd_Arab'\n",
        "        }\n",
        "\n",
        "    def pre_process(self, input):\n",
        "        # pre-process the input in the same way as we pro-process the training sample\n",
        "        return input\n",
        "\n",
        "\n",
        "    def char_percent_check(self, input):\n",
        "        # check whether input has input_threhsold of roman characters\n",
        "\n",
        "        # count total number of characters in string\n",
        "        input_len = len(list(input))\n",
        "\n",
        "        # count special character spaces and newline in string\n",
        "        special_char_pattern = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
        "        special_char_matches = special_char_pattern.findall(input)\n",
        "        special_chars = len(special_char_matches)\n",
        "\n",
        "        spaces = len(re.findall('\\s', input))\n",
        "        newlines = len(re.findall('\\n', input))\n",
        "\n",
        "        # subtract total-special character counts\n",
        "        total_chars = input_len - (special_chars + spaces + newlines)\n",
        "\n",
        "        # count the number of english character and digit in string\n",
        "        en_pattern = re.compile('[a-zA-Z0-9]')\n",
        "        en_matches = en_pattern.findall(input)\n",
        "        en_chars = len(en_matches)\n",
        "\n",
        "        # calculate the percentage of english character in total number of characters\n",
        "        if total_chars == 0:\n",
        "            return 0\n",
        "        return (en_chars/total_chars)\n",
        "\n",
        "\n",
        "\n",
        "    def native_inference(self, input_list, output_dict):\n",
        "\n",
        "        if not input_list:\n",
        "            return output_dict\n",
        "\n",
        "        # inference for fasttext native script model\n",
        "        input_texts = [line[1] for line in input_list]\n",
        "        IndicLID_FTN_predictions = self.IndicLID_FTN.predict(input_texts)\n",
        "\n",
        "        # add result of input directly to output_dict\n",
        "        for input, pred_label, pred_score in zip(input_list, IndicLID_FTN_predictions[0], IndicLID_FTN_predictions[1]):\n",
        "            # print(pred_score)\n",
        "            output_dict[input[0]] = (input[1], pred_label[0][9:], pred_score[0], 'IndicLID-FTN')\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    def roman_inference(self, input_list, output_dict, batch_size):\n",
        "\n",
        "        if not input_list:\n",
        "            return output_dict\n",
        "\n",
        "        # 1st stage\n",
        "        # inference for fasttext roman script model\n",
        "        input_texts = [line[1] for line in input_list]\n",
        "        IndicLID_FTR_predictions = self.IndicLID_FTR.predict(input_texts)\n",
        "\n",
        "        IndicLID_BERT_inputs = []\n",
        "        # add result of input directly to output_dict\n",
        "        for input, pred_label, pred_score in zip(input_list, IndicLID_FTR_predictions[0], IndicLID_FTR_predictions[1]):\n",
        "            if pred_score[0] > self.model_threshold:\n",
        "                output_dict[input[0]] = (input[1], pred_label[0][9:], pred_score[0], 'IndicLID-FTR')\n",
        "            else:\n",
        "                IndicLID_BERT_inputs.append(input)\n",
        "\n",
        "        # 2nd stage\n",
        "        output_dict = self.IndicBERT_roman_inference(IndicLID_BERT_inputs, output_dict, batch_size)\n",
        "        return output_dict\n",
        "\n",
        "\n",
        "    def IndicBERT_roman_inference(self, IndicLID_BERT_inputs, output_dict, batch_size):\n",
        "        # inference for IndicBERT roman script model\n",
        "\n",
        "        if not IndicLID_BERT_inputs:\n",
        "            return output_dict\n",
        "\n",
        "        df = pd.DataFrame(IndicLID_BERT_inputs)\n",
        "        dataloader = self.get_dataloaders(df.iloc[:,0], df.iloc[:,1], batch_size)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in dataloader:\n",
        "                batch_indices = data[0]\n",
        "                batch_inputs = data[1]\n",
        "\n",
        "                word_embeddings = self.IndicLID_BERT_tokenizer(batch_inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "                word_embeddings = word_embeddings.to(self.device)\n",
        "\n",
        "                batch_outputs = self.IndicLID_BERT(word_embeddings['input_ids'],\n",
        "                            token_type_ids=word_embeddings['token_type_ids'],\n",
        "                            attention_mask=word_embeddings['attention_mask']\n",
        "                            )\n",
        "\n",
        "\n",
        "                _, batch_predicted = torch.max(batch_outputs.logits, 1)\n",
        "\n",
        "\n",
        "                for index, input, pred_label, logit in zip(batch_indices, batch_inputs, batch_predicted, batch_outputs.logits):\n",
        "                    output_dict[index] = (input,\n",
        "                                            self.IndicLID_lang_code_dict_reverse[pred_label.item()],\n",
        "                                            logit[pred_label.item()].item(), 'IndicLID-BERT'\n",
        "                                            )\n",
        "\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "\n",
        "    def post_process(self, output_dict):\n",
        "        # output the result in some consistent language code format\n",
        "        results = []\n",
        "        keys = list(output_dict.keys())\n",
        "        keys.sort()\n",
        "        for index in keys:\n",
        "            results.append( output_dict[index] )\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_dataloaders(self, indices, input_texts, batch_size):\n",
        "        data_obj = IndicBERT_Data(indices, input_texts)\n",
        "        dl = torch.utils.data.DataLoader(data_obj,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=False\n",
        "                                                )\n",
        "        return dl\n",
        "\n",
        "    def predict(self, input):\n",
        "        input_list = [input,]\n",
        "        self.batch_predict(input_list, 1)\n",
        "\n",
        "    def batch_predict(self, input_list, batch_size):\n",
        "\n",
        "        # call functions seq by seq and divert the input to IndicBERT if\n",
        "        # fasttext prediction score is less than the defined model_threhsold.\n",
        "        # Also output the inference time along with the result.\n",
        "        output_dict = {}\n",
        "\n",
        "        roman_inputs = []\n",
        "        native_inputs = []\n",
        "\n",
        "        # text roman percent check\n",
        "        for index, input in enumerate(input_list):\n",
        "            if self.char_percent_check(input) > self.input_threshold:\n",
        "                roman_inputs.append((index, input))\n",
        "            else:\n",
        "                native_inputs.append((index, input))\n",
        "\n",
        "        output_dict = self.native_inference(native_inputs, output_dict)\n",
        "        output_dict = self.roman_inference(roman_inputs, output_dict, batch_size)\n",
        "\n",
        "        results = self.post_process(output_dict)\n",
        "        return results"
      ],
      "metadata": {
        "id": "irD899iQMAGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjyY3XXUDSRG"
      },
      "outputs": [],
      "source": [
        "IndicLID_model = IndicLID(input_threshold = 0.5, roman_lid_threshold = 0.6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNjEXINmDRZ4"
      },
      "outputs": [],
      "source": [
        "test_samples = [\n",
        "   'आज के दिन का मौसम अत्यंत सुंदर है, जहां सदैव छाए हुए बादल, गुलाबी रंगीन शाम, और हल्की हवा के साथ प्राकृतिक सौंदर्य का आनंद लेने का एक सुनहरा अवसर है',\n",
        "   'aaj key din ka mausam atyant sundar hai, jahan sadaiv chae hue baadal, gulabi rangeen shaam, aur halki havaa key saath praakritik saundarya kaa anand lene kaa aeka sunhara avsar haye',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5QZzN3dHoO0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "batch_size = 1\n",
        "outputs = IndicLID_model.batch_predict(test_samples, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fVTr-v2mH0p"
      },
      "outputs": [],
      "source": [
        "print(IndicLID_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mOLTezPHBs1"
      },
      "outputs": [],
      "source": [
        "print(outputs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}