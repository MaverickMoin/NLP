{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LIBRARIES"
      ],
      "metadata": {
        "id": "UjiU4MYel-Ft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUs63KOEBSiz"
      },
      "outputs": [],
      "source": [
        "!pip3 install fasttext\n",
        "!pip3 install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLONING INDICLID REPO FROM GITHUB"
      ],
      "metadata": {
        "id": "7wOR01N0mDFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gDhqkoTnUHbh"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AI4Bharat/IndicLID.git\n",
        "%cd \"/content/IndicLID/Inference\"\n",
        "%mkdir models\n",
        "%cd \"/content/IndicLID/Inference/models\"\n",
        "\n",
        "\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-bert.zip\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-ftn.zip\n",
        "!wget https://github.com/AI4Bharat/IndicLID/releases/download/v1.0/indiclid-ftr.zip\n",
        "!unzip indiclid-bert.zip\n",
        "!unzip indiclid-ftn.zip\n",
        "!unzip indiclid-ftr.zip\n",
        "\n",
        "%cd \"/content/IndicLID/\"\n",
        "%mkdir train_data\n",
        "%cd \"/content/IndicLID/train_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLING INDICXLIT"
      ],
      "metadata": {
        "id": "tF8GG3UJmJn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ3uBDprVFBR"
      },
      "outputs": [],
      "source": [
        "!python -m pip install \"pip<24.1\"\n",
        "!pip install ai4bharat-transliteration\n",
        "\n",
        "from ai4bharat.transliteration import XlitEngine\n",
        "\n",
        "e = XlitEngine(beam_width=10, src_script_type = \"indic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FETCHING NATIVE DATA FROM INDICCORPV2 DATASET"
      ],
      "metadata": {
        "id": "uLS-V77amNx9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaaE5Dv7chkF"
      },
      "outputs": [],
      "source": [
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/as.txt | head -n 30000 > as.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/bd.txt | head -n 30000 > bd.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/bn.txt | head -n 30000 > bn.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/en.txt | head -n 30000 > en.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/gom.txt | head -n 30000 > gom.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/gu.txt | head -n 30000 > gu.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/hi.txt | head -n 30000 > hi.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/kn.txt | head -n 30000 > kn.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ks.txt | head -n 30000 > ks.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mai.txt | head -n 30000 > mai.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ml.txt | head -n 30000 > ml.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mni.txt | head -n 30000 > mni.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/mr.txt | head -n 30000 > mr.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ne.txt | head -n 30000 > ne.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/or.txt | head -n 30000 > or.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/pa.txt | head -n 30000 > pa.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/sa.txt | head -n 30000 > sa.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/sd.txt | head -n 30000 > sd.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ta.txt | head -n 30000 > ta.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/te.txt | head -n 30000 > te.txt\n",
        "!curl -s https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/ur.txt | head -n 30000 > ur.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DICTIONARY FROM GENERATING TRANSLIERATED DATA"
      ],
      "metadata": {
        "id": "CgLKbkqjmWsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZmkc7uru3S6"
      },
      "outputs": [],
      "source": [
        "lang_code_dict = {\n",
        "    'as' : 'asm_Latn' ,\n",
        "    'bn' : 'ben_Latn' ,\n",
        "    'bd' : 'brx_Latn' ,\n",
        "    'gom': 'kok_Latn' ,\n",
        "    'gu' : 'guj_Latn' ,\n",
        "    'hi' : 'hin_Latn' ,\n",
        "    'kn' : 'kan_Latn' ,\n",
        "    'ks' : 'kas_Latn' ,\n",
        "    'mai': 'mai_Latn' ,\n",
        "    'ml' : 'mal_Latn' ,\n",
        "    'mni': 'mni_Latn' ,\n",
        "    'mr' : 'mar_Latn' ,\n",
        "    'ne' : 'nep_Latn' ,\n",
        "    'or' : 'ori_Latn' ,\n",
        "    'pa' : 'pan_Latn' ,\n",
        "    'sa' : 'san_Latn' ,\n",
        "    'sd' : 'snd_Latn' ,\n",
        "    'ta' : 'tam_Latn' ,\n",
        "    'te' : 'tel_Latn' ,\n",
        "    'ur' : 'urd_Latn' ,\n",
        "}\n",
        "\n",
        "xlit_code_dict = {\n",
        "    'as' : 'asm_Latn' ,\n",
        "    'bn' : 'ben_Latn' ,\n",
        "    'bd' : 'brx_Latn' ,\n",
        "    'gom': 'kok_Latn' ,\n",
        "    'gu' : 'guj_Latn' ,\n",
        "    'hi' : 'hin_Latn' ,\n",
        "    'kn' : 'kan_Latn' ,\n",
        "    'ks' : 'kas_Latn' ,\n",
        "    'mai': 'mai_Latn' ,\n",
        "    'ml' : 'mal_Latn' ,\n",
        "    'mni': 'mni_Latn' ,\n",
        "    'mr' : 'mar_Latn' ,\n",
        "    'ne' : 'nep_Latn' ,\n",
        "    'or' : 'ori_Latn' ,\n",
        "    'pa' : 'pan_Latn' ,\n",
        "    'sa' : 'san_Latn' ,\n",
        "    'sd' : 'snd_Latn' ,\n",
        "    'ta' : 'tam_Latn' ,\n",
        "    'te' : 'tel_Latn' ,\n",
        "    'ur' : 'urd_Latn' ,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING TRANSLITERATED DATA USING INDICXLIT"
      ],
      "metadata": {
        "id": "PEF4VL7KmbmD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J5W7P6pngRs"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "for code, lang in lang_code_dict.items():\n",
        "\n",
        "\t# Traverse the file line by line and transliterate\n",
        "\tfile_path = f\"/content/IndicLID/train_data/{code}.txt\"\n",
        "\toutput_file_path = f\"/content/IndicLID/train_data/{code}_train.txt\"\n",
        "\toutput_file = open(output_file_path, 'w', encoding='utf-8')\n",
        "\ti=0\n",
        "\t# Read the file and transliterate each line\n",
        "\twith open(file_path, 'r', encoding='utf-8') as file:\n",
        "\t\tfor line in file:\n",
        "\t\t\tif i<20000:\n",
        "\t\t\t\t# Strip any whitespace or newline characters from the line\n",
        "\t\t\t\tline = line.lower()\n",
        "\t\t\t\tline = re.sub(r\"[^\\w\\s,.'\\n]\", '', line)\n",
        "\t\t\t\tline = re.sub(r'\\s+', ' ', line).strip()\n",
        "\t\t\t\ti+=1\n",
        "\t\t\t\t# Transliterate the line using the engine\n",
        "\t\t\t\tif line:  # Ensure it's not an empty line\n",
        "\t\t\t\t\txlit_code = 'brx' if code == 'bd' else code\n",
        "\t\t\t\t\toutput = e.translit_sentence(line, xlit_code)\n",
        "\t\t\t\t\toutput_file.write(f\"__label__{lang} {output}\\n\")\n",
        "\t\t\t\t\tprint(f\"Input: {line}\")\n",
        "\t\t\t\t\tprint(f\"__label__{lang} {output}\")\n",
        "\n",
        "\toutput_file.close()  # Manually closing the file after the loop\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls *_train.txt\n",
        "!cat *_train.txt > train_combine.txt"
      ],
      "metadata": {
        "id": "eBPek1JAWcfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT FASTTEXT AND BUILDING MODEL"
      ],
      "metadata": {
        "id": "GPn_LeWzmhzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slCloT41LrYp"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "model = fasttext.load_model('/content/IndicLID/Inference/models/indiclid-ftr/model_baseline_roman.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINETUNING FASTTEXT FOR ROMANISED DATA"
      ],
      "metadata": {
        "id": "1BsMZ4VTmmUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJM1b_frL3B4"
      },
      "outputs": [],
      "source": [
        "train_data= f\"/content/IndicLID/train_data/train_combine.txt\"\n",
        "print(train_data)\n",
        "model = fasttext.train_supervised(\n",
        "    input = train_data,\n",
        "    lr = 0.3,\n",
        "    dim = 512,\n",
        "    ws = 5,\n",
        "    epoch = 4,\n",
        "    minCount = 1,\n",
        "    minCountLabel = 0,\n",
        "    minn = 3,\n",
        "    maxn = 6,\n",
        "    neg = 5,\n",
        "    wordNgrams = 2,\n",
        "    loss = 'hs',\n",
        "    lrUpdateRate = 100,\n",
        "    t = 0.0001,\n",
        "    verbose = 1\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVING THE FINETUNED MODEL PARAMETERS"
      ],
      "metadata": {
        "id": "FFApBodjmt6s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KFwuSbzvLaP"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/IndicLID/\"\n",
        "%mkdir finetuned\n",
        "%mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gwpYtxbHRs1"
      },
      "outputs": [],
      "source": [
        "model.save_model(\"/content/IndicLID/finetuned/model_baseline_roman.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATING ORIGINAL MODEL AND FINETUNED MODEL AGAINST SUBSET OF THE SAME DATA AS USED IN THE PAPER"
      ],
      "metadata": {
        "id": "4O5wn4m4m5yR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uZq-C8-5gNR"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import sys\n",
        "\n",
        "IndicPath='/content/IndicLID/Inference/models/indiclid-ftr/model_baseline_roman.bin'\n",
        "FinetunePath='/content/IndicLID/finetuned/model_baseline_roman.bin'\n",
        "model = fasttext.load_model(FinetunePath)\n",
        "\n",
        "file = open('/content/IndicLID/results/result.txt', 'w')\n",
        "\n",
        "lines_config = []\n",
        "\n",
        "lines_config.append('lr : ' + str ( model.f.getArgs().lr ) )\n",
        "lines_config.append('dim : ' + str ( model.f.getArgs().dim ) )\n",
        "lines_config.append('ws : ' + str ( model.f.getArgs().ws ) )\n",
        "lines_config.append('epoch : ' + str ( model.f.getArgs().epoch ) )\n",
        "lines_config.append('minCount : ' + str ( model.f.getArgs().minCount ) )\n",
        "lines_config.append('minCountLabel : ' + str ( model.f.getArgs().minCountLabel ) )\n",
        "lines_config.append('minn : ' + str ( model.f.getArgs().minn ) )\n",
        "lines_config.append('maxn : ' + str ( model.f.getArgs().maxn ) )\n",
        "lines_config.append('neg : ' + str ( model.f.getArgs().neg ) )\n",
        "lines_config.append('wordNgrams : ' + str ( model.f.getArgs().wordNgrams ) )\n",
        "lines_config.append('loss : ' + str ( model.f.getArgs().loss ) )\n",
        "lines_config.append('bucket : ' + str ( model.f.getArgs().bucket ) )\n",
        "lines_config.append('thread : ' + str ( model.f.getArgs().thread ) )\n",
        "lines_config.append('lrUpdateRate : ' + str ( model.f.getArgs().lrUpdateRate ) )\n",
        "lines_config.append('t : ' + str ( model.f.getArgs().t ) )\n",
        "lines_config.append('label : ' + str ( model.f.getArgs().label ) )\n",
        "lines_config.append('verbose : ' + str ( model.f.getArgs().verbose ) )\n",
        "lines_config.append('pretrainedVectors : ' + str ( model.f.getArgs().pretrainedVectors ) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "file.write('Hyperparameters (Tuned)')\n",
        "file.write('\\n')\n",
        "file.write('\\n'.join(lines_config))\n",
        "file.write('\\n\\n\\n\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "result_train = model.test(\"/content/test_combine.txt\")\n",
        "\n",
        "file.write('Evaluation Results Train Set')\n",
        "file.write('\\n')\n",
        "file.write('train Set')\n",
        "file.write('\\n')\n",
        "file.write('Samples : ' + str(result_train[0]) )\n",
        "file.write('\\n')\n",
        "file.write('precision : ' + str(result_train[1]) )\n",
        "file.write('\\n')\n",
        "file.write('recall : ' + str(result_train[2]) )\n",
        "file.write('\\n')\n",
        "file.write('F1-Score : ' + str( (2 * result_train[1] * result_train[2]) / (result_train[1] + result_train[2]) ) )\n",
        "file.write('\\n\\n\\n')\n",
        "\n",
        "\n",
        "confusion_matrix_reverse_mapping  = {\n",
        "    0 : 'Assamese',\n",
        "    1 : 'Bangla',\n",
        "    2 : 'Bodo',\n",
        "    3 : 'Konkani',\n",
        "    4 : 'Gujarati',\n",
        "    5 : 'Hindi',\n",
        "    6 : 'Kannada',\n",
        "    7 : 'Kashmiri',\n",
        "    8 : 'Maithili',\n",
        "    9 : 'Malayalam',\n",
        "    10 : 'Manipuri',\n",
        "    11 : 'Marathi',\n",
        "    12 : 'Nepali',\n",
        "    13 : 'Oriya',\n",
        "    14 : 'Punjabi',\n",
        "    15 : 'Sanskrit',\n",
        "    16 : 'Sindhi',\n",
        "    17 : 'Tamil',\n",
        "    18 : 'Telugu',\n",
        "    19 : 'Urdu',\n",
        "    20 : 'English',\n",
        "    21 : 'Other'\n",
        "}\n",
        "\n",
        "confusion_matrix_mapping  = {\n",
        "    'Assamese' : 0,\n",
        "    'Bangla' : 1,\n",
        "    'Bodo' : 2,\n",
        "    'Konkani' : 3,\n",
        "    'Gujarati' : 4,\n",
        "    'Hindi' : 5,\n",
        "    'Kannada' : 6,\n",
        "    'Kashmiri' : 7,\n",
        "    'Maithili' : 8,\n",
        "    'Malayalam' : 9,\n",
        "    'Manipuri' : 10,\n",
        "    'Marathi' : 11,\n",
        "    'Nepali' : 12,\n",
        "    'Oriya' : 13,\n",
        "    'Punjabi' : 14,\n",
        "    'Sanskrit' : 15,\n",
        "    'Sindhi' : 16,\n",
        "    'Tamil' : 17,\n",
        "    'Telugu' : 18,\n",
        "    'Urdu' : 19,\n",
        "    'English' : 20,\n",
        "    'Other' : 21\n",
        "}\n",
        "\n",
        "IndicLID_lang_code_dict = {\n",
        "    'asm_Latn' : 0,\n",
        "    'ben_Latn' : 1,\n",
        "    'brx_Latn' : 2,\n",
        "\t'kok_Latn' : 3,\n",
        "    'guj_Latn' : 4,\n",
        "    'hin_Latn' : 5,\n",
        "    'kan_Latn' : 6,\n",
        "    'kas_Latn' : 7,\n",
        "    'mai_Latn' : 8,\n",
        "    'mal_Latn' : 9,\n",
        "    'mni_Latn' : 10,\n",
        "    'mar_Latn' : 11,\n",
        "    'nep_Latn' : 12,\n",
        "    'ori_Latn' : 13,\n",
        "    'pan_Latn' : 14,\n",
        "    'san_Latn' : 15,\n",
        "    'snd_Latn' : 16,\n",
        "    'tam_Latn' : 17,\n",
        "    'tel_Latn' : 18,\n",
        "    'urd_Latn' : 19,\n",
        "    'eng_Latn' : 20,\n",
        "    'other' : 21,\n",
        "    'asm_Beng' : 22,\n",
        "    'ben_Beng' : 23,\n",
        "    'brx_Deva' : 24,\n",
        "    'doi_Deva' : 25,\n",
        "    'guj_Gujr' : 26,\n",
        "    'hin_Deva' : 27,\n",
        "    'kan_Knda' : 28,\n",
        "    'kas_Arab' : 29,\n",
        "    'kas_Deva' : 30,\n",
        "    'kok_Deva' : 31,\n",
        "    'mai_Deva' : 32,\n",
        "    'mal_Mlym' : 33,\n",
        "    'mni_Beng' : 34,\n",
        "    'mni_Meti' : 35,\n",
        "    'mar_Deva' : 36,\n",
        "    'nep_Deva' : 37,\n",
        "    'ori_Orya' : 38,\n",
        "    'pan_Guru' : 39,\n",
        "    'san_Deva' : 40,\n",
        "    'sat_Olch' : 41,\n",
        "    'snd_Arab' : 42,\n",
        "    'tam_Tamil' : 43,\n",
        "    'tel_Telu' : 44,\n",
        "    'urd_Arab' : 45,\n",
        "    #'mni_Latn_Mei' : 10,\n",
        "    #'Other' : 21,\n",
        "    #'English' : 20\n",
        "\n",
        "}\n",
        "\n",
        "IndicLID_lang_code_dict_reverse = {\n",
        "    0 : 'asm_Latn',\n",
        "    1 : 'ben_Latn',\n",
        "    2 : 'brx_Latn',\n",
        "    4 : 'guj_Latn',\n",
        "    5 : 'hin_Latn',\n",
        "    6 : 'kan_Latn',\n",
        "    7 : 'kas_Latn',\n",
        "    3 : 'kok_Latn',\n",
        "    8 : 'mai_Latn',\n",
        "    9 : 'mal_Latn',\n",
        "    10 : 'mni_Latn',\n",
        "    11 : 'mar_Latn',\n",
        "    12 : 'nep_Latn',\n",
        "    13 : 'ori_Latn',\n",
        "    14 : 'pan_Latn',\n",
        "    15 : 'san_Latn',\n",
        "    16 : 'snd_Latn',\n",
        "    17 : 'tam_Latn',\n",
        "    18 : 'tel_Latn',\n",
        "    19 : 'urd_Latn',\n",
        "    20 : 'eng_Latn',\n",
        "    21 : 'other',\n",
        "    22 : 'asm_Beng',\n",
        "    23 : 'ben_Beng',\n",
        "    24 : 'brx_Deva',\n",
        "    25 : 'doi_Deva',\n",
        "    26 : 'guj_Gujr',\n",
        "    27 : 'hin_Deva',\n",
        "    28 : 'kan_Knda',\n",
        "    29 : 'kas_Arab',\n",
        "    30 : 'kas_Deva',\n",
        "    31 : 'kok_Deva',\n",
        "    32 : 'mai_Deva',\n",
        "    33 : 'mal_Mlym',\n",
        "    34 : 'mni_Beng',\n",
        "    35 : 'mni_Meti',\n",
        "    36 : 'mar_Deva',\n",
        "    37 : 'nep_Deva',\n",
        "    38 : 'ori_Orya',\n",
        "    39 : 'pan_Guru',\n",
        "    40 : 'san_Deva',\n",
        "    41 : 'sat_Olch',\n",
        "    42 : 'snd_Arab',\n",
        "    43 : 'tam_Tamil',\n",
        "    44 : 'tel_Telu',\n",
        "    45 : 'urd_Arab'\n",
        "}\n",
        "\n",
        "\n",
        "def evaluate(test_file_name):\n",
        "\n",
        "    classes = 22\n",
        "\n",
        "    # inference for Dakshina, test_combine_dakshina\n",
        "    file_test = open('/content/'+test_file_name+'.txt', 'r')\n",
        "    lines_test = file_test.read().split('\\n')\n",
        "    file_test.close()\n",
        "\n",
        "\n",
        "    # save predictions\n",
        "    file_predictions = open('/content/IndicLID/results/predictions_'+test_file_name+'.csv', 'w')\n",
        "    csv_writer_predictions = csv.writer(file_predictions)\n",
        "    csv_writer_predictions.writerow( [ 'Sentence', 'Ground truth', 'Prediction', 'Score' ] )\n",
        "\n",
        "    file_predictions_right = open('/content/IndicLID/results/right_predictions_'+test_file_name+'.csv', 'w')\n",
        "    csv_writer_predictions_right = csv.writer(file_predictions_right)\n",
        "    csv_writer_predictions_right.writerow( [ 'Sentence', 'Ground truth', 'Prediction', 'Score' ] )\n",
        "\n",
        "\n",
        "    file_predictions_wrong = open('/content/IndicLID/results/wrong_predictions_'+test_file_name+'.csv', 'w')\n",
        "    csv_writer_predictions_wrong = csv.writer(file_predictions_wrong)\n",
        "    csv_writer_predictions_wrong.writerow( [ 'Sentence', 'Ground truth', 'Prediction', 'Score' ] )\n",
        "\n",
        "\n",
        "\n",
        "    # Computing confusion matrix\n",
        "    confusion_matrix = []\n",
        "    for i in range(classes):\n",
        "        confusion_matrix.append( [0]*classes )\n",
        "\n",
        "    # to calculate accuracy and save right and wrong prediction\n",
        "    count = 0\n",
        "    n = 0\n",
        "    for line in lines_test:\n",
        "        label = line.split(' ')[0]\n",
        "        sen = ' '.join(line.split(' ')[1:])\n",
        "        pred_label = model.predict(sen)[0][0]\n",
        "        pred_score = model.predict(sen)[1][0]\n",
        "\n",
        "        if pred_label == label:\n",
        "            count+=1\n",
        "            csv_writer_predictions_right.writerow( [ sen, label, pred_label, pred_score ] )\n",
        "        else:\n",
        "            csv_writer_predictions_wrong.writerow( [ sen, label, pred_label, pred_score ] )\n",
        "        n+=1\n",
        "        #print(\"This is label : \"+label)\n",
        "        #print(\"This is label : \"+pred_label)\n",
        "\n",
        "\n",
        "        if len(label[9:])>0:\n",
        "          print(\"This file label : \"+label[9:])\n",
        "          print(\"This model label : \"+pred_label[9:])\n",
        "          confusion_matrix[ IndicLID_lang_code_dict[label[9:]] ][ IndicLID_lang_code_dict[pred_label[9:]] ] += 1\n",
        "          csv_writer_predictions.writerow( [ sen, label, pred_label, pred_score ] )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Computing precision, recall and f1\n",
        "    precsison_recall_f1 = []\n",
        "    for i in range(classes):\n",
        "        precsison_recall_f1.append([0] * 3)\n",
        "\n",
        "    precision_denominator = 0\n",
        "    recall_denominator = 0\n",
        "    f1_denominator = 0\n",
        "\n",
        "    for i in range(classes):\n",
        "        no_of_correctly_predicted = confusion_matrix[i][i]\n",
        "        total_predictions_as_i = 0\n",
        "\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1_value = 0\n",
        "\n",
        "        # true predicted i values out of all predicted i values\n",
        "        for j in range(classes):\n",
        "            total_predictions_as_i += confusion_matrix[j][i]\n",
        "        if (total_predictions_as_i != 0):\n",
        "            precision = no_of_correctly_predicted/total_predictions_as_i\n",
        "            precision_denominator += 1\n",
        "\n",
        "        # true predicted i values out of all actual i values\n",
        "        total_actual_values_of_i = sum(confusion_matrix[i])\n",
        "        if (total_actual_values_of_i != 0):\n",
        "            recall = no_of_correctly_predicted/total_actual_values_of_i\n",
        "            recall_denominator += 1\n",
        "\n",
        "        # f1 score\n",
        "        if (precision + recall != 0):\n",
        "            f1_value = (2 * precision * recall) / (precision + recall)\n",
        "            f1_denominator += 1\n",
        "\n",
        "        precsison_recall_f1[i][0] = precision\n",
        "        precsison_recall_f1[i][1] = recall\n",
        "        precsison_recall_f1[i][2] = f1_value\n",
        "\n",
        "    avg_precision = sum([precsison_recall_f1[i][0] for i in range(classes)]) / precision_denominator\n",
        "    avg_recall = sum([precsison_recall_f1[i][1] for i in range(classes)]) / recall_denominator\n",
        "    avg_f1_score = sum([precsison_recall_f1[i][2] for i in range(classes)]) / f1_denominator\n",
        "\n",
        "\n",
        "\n",
        "    # to save confusion matrix and precision recall matrix\n",
        "\n",
        "    for i in range(classes):\n",
        "        precsison_recall_f1[i].insert(0, confusion_matrix_reverse_mapping[i])\n",
        "\n",
        "    precsison_recall_f1.insert( 0, ['', 'precision', 'recall', 'f1'])\n",
        "    precsison_recall_f1.append( ['Avg', avg_precision, avg_recall, avg_f1_score] )\n",
        "\n",
        "\n",
        "    file_precision_recall_f1 = open('/content/IndicLID/results/precision_recall_f1_'+test_file_name+'.csv', 'w')\n",
        "    precision_recall_f1_csv_writer = csv.writer(file_precision_recall_f1)\n",
        "\n",
        "    for i in range(classes+2):\n",
        "        precision_recall_f1_csv_writer.writerow(precsison_recall_f1[i])\n",
        "    file_precision_recall_f1.close()\n",
        "\n",
        "\n",
        "\n",
        "    # save confusion matrix\n",
        "    for i in range(classes):\n",
        "        confusion_matrix[i].insert(0, confusion_matrix_reverse_mapping[i] )\n",
        "\n",
        "    confusion_matrix.insert( 0, [''] + [confusion_matrix_reverse_mapping[i] for i in range(classes)] )\n",
        "\n",
        "\n",
        "    file_confusion_matrix = open('/content/IndicLID/results/confusion_matrix_'+test_file_name+'.csv', 'w')\n",
        "    confusion_matrix_csv_writer = csv.writer(file_confusion_matrix)\n",
        "\n",
        "    for i in range(classes+1):\n",
        "        confusion_matrix_csv_writer.writerow(confusion_matrix[i])\n",
        "\n",
        "\n",
        "    file_confusion_matrix.close()\n",
        "    file_predictions.close()\n",
        "    file_predictions_right.close()\n",
        "    file_predictions_wrong.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # fasttext evaluation scores\n",
        "    test_acc = count/n\n",
        "    result_test = model.test('/content/test_combine.txt')\n",
        "    file.write('fasttext evaluation scores - ' + test_file_name + '\\n')\n",
        "    file.write('Samples : ' + str(result_test[0]) + '\\n')\n",
        "    file.write('precision : ' + str(result_test[1]) + '\\n')\n",
        "    file.write('recall : ' + str(result_test[2]) + '\\n')\n",
        "    file.write('F1-Score : ' + str( (2 * result_test[1] * result_test[2]) / (result_test[1] + result_test[2]) ) + '\\n')\n",
        "    file.write('Accuracy : ' + str(test_acc) + '\\n')\n",
        "    file.write('\\n\\n\\n')\n",
        "\n",
        "evaluate('test_combine')\n",
        "\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFUSION MATRIX FOR INDICLID FTR"
      ],
      "metadata": {
        "id": "4PKo8ldTnLVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuTovpKbVVqp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the confusion matrix from a CSV file\n",
        "file_path = '/content/IndicLID/results/confusion_matrix_test_combine.csv'  # Update with your file path\n",
        "confusion_matrix_df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the dataframe to set the first column as an index (assuming it contains row labels)\n",
        "confusion_matrix = confusion_matrix_df.set_index(confusion_matrix_df.columns[0])\n",
        "\n",
        "# Plotting the heatmap using Seaborn\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(confusion_matrix, annot=False, cmap=\"Blues\", cbar=True)\n",
        "plt.title(\"Confusion Matrix Heatmap (Shades)\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalised Confusion Matrix FOR INDICLID FTR(Recall)"
      ],
      "metadata": {
        "id": "UXu5wFdjcOXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the confusion matrix from a CSV file\n",
        "file_path = '/content/IndicLID/results/confusion_matrix_test_combine.csv'  # Update with your file path\n",
        "confusion_matrix_df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the dataframe to set the first column as an index (assuming it contains row labels)\n",
        "confusion_matrix = confusion_matrix_df.set_index(confusion_matrix_df.columns[0])\n",
        "\n",
        "#Diagonal is True Positive / (True Positive+False Negative)\n",
        "confusion_matrix_col_norm = confusion_matrix.div(confusion_matrix.sum(axis=0), axis=1)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(confusion_matrix_col_norm, annot=False, cmap=\"Blues\", cbar=True)\n",
        "plt.title(\"Column-Normalized Confusion Matrix Heatmap\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y5TK4SjabinZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRECISION-RECALL PARAMETERS FOR INDICLID FTR"
      ],
      "metadata": {
        "id": "T30sbeg0nPcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-Vt633lWT2H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "file_path = '/content/IndicLID/results/precision_recall_f1_test_combine.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "data.columns = [column.upper() for column in data.columns]\n",
        "\n",
        "# Display the data as a styled table with solid borders\n",
        "styled_table = data.style.set_table_styles(\n",
        "    [{'selector': 'table', 'props': [('border-collapse', 'collapse')]},\n",
        "     {'selector': 'th', 'props': [('border', '1px solid black'), ('padding', '8px')]},\n",
        "     {'selector': 'td', 'props': [('border', '1px solid black'), ('padding', '8px')]}]\n",
        ").set_properties(**{'text-align': 'center'})\n",
        "\n",
        "# Render the styled table\n",
        "styled_table\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}